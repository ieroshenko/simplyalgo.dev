import { useState, useRef, useCallback, useEffect } from "react";

interface UseRealtimeInterviewProps {
  resumeText: string;
  voice: string;
  onConnectionStatusChange: (status: "disconnected" | "connecting" | "connected") => void;
  onTranscript?: (role: "user" | "assistant", content: string) => void;
}

interface UseRealtimeInterviewReturn {
  startInterview: () => Promise<void>;
  stopInterview: () => void;
  audioAnalyser: AnalyserNode | null;
  error: string | null;
}

export const useRealtimeInterview = ({
  resumeText,
  voice,
  onConnectionStatusChange,
  onTranscript,
}: UseRealtimeInterviewProps): UseRealtimeInterviewReturn => {
  const [audioAnalyser, setAudioAnalyser] = useState<AnalyserNode | null>(null);
  const [error, setError] = useState<string | null>(null);

  const peerConnectionRef = useRef<RTCPeerConnection | null>(null);
  const dataChannelRef = useRef<RTCDataChannel | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const localStreamRef = useRef<MediaStream | null>(null);
  const remoteAudioRef = useRef<HTMLAudioElement | null>(null);

  // Cleanup function
  const cleanup = useCallback(() => {
    console.log("[Realtime] Cleaning up resources...");

    // Close data channel
    if (dataChannelRef.current) {
      dataChannelRef.current.close();
      dataChannelRef.current = null;
    }

    // Close peer connection
    if (peerConnectionRef.current) {
      peerConnectionRef.current.close();
      peerConnectionRef.current = null;
    }

    // Stop local media tracks
    if (localStreamRef.current) {
      localStreamRef.current.getTracks().forEach((track) => track.stop());
      localStreamRef.current = null;
    }

    // Close audio context
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    if (remoteAudioRef.current) {
      remoteAudioRef.current.srcObject = null;
      remoteAudioRef.current.remove();
      remoteAudioRef.current = null;
    }

    setAudioAnalyser(null);
    onConnectionStatusChange("disconnected");
  }, [onConnectionStatusChange]);

  // Start interview
  const startInterview = useCallback(async () => {
    try {
      setError(null);
      onConnectionStatusChange("connecting");
      console.log("[Realtime] Starting interview...");

      // Get ephemeral token from backend
      console.log("[Realtime] Requesting ephemeral token...");
      const tokenResponse = await fetch("/api/ephemeral-token", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: "gpt-realtime",
          voice: voice,
        }),
      });

      if (!tokenResponse.ok) {
        const errorData = await tokenResponse.json();
        throw new Error(errorData.error || "Failed to get ephemeral token");
      }

      const { token } = await tokenResponse.json();
      console.log("[Realtime] Received ephemeral token");

      // Create peer connection
      const pc = new RTCPeerConnection();
      peerConnectionRef.current = pc;

      // Set up audio context for visualization
      const audioContext = new AudioContext();
      if (audioContext.state === "suspended") {
        await audioContext.resume().catch(() => undefined);
      }
      audioContextRef.current = audioContext;

      // Get user media (microphone)
      console.log("[Realtime] Requesting microphone access...");
      const localStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
      });
      localStreamRef.current = localStream;

      // Add local audio track to peer connection
      localStream.getTracks().forEach((track) => {
        pc.addTrack(track, localStream);
      });
      console.log("[Realtime] Local audio track added");

      // Create data channel for control events
      const dataChannel = pc.createDataChannel("oai-events");
      dataChannelRef.current = dataChannel;

      // Handle data channel events
      dataChannel.addEventListener("open", () => {
        console.log("[Realtime] Data channel opened");

        // Send session configuration
        const sessionConfig = {
          type: "session.update",
          session: {
            modalities: ["text", "audio"],
            voice,
            input_audio_transcription: {
              model: "whisper-1"
            },
            turn_detection: {
              type: "server_vad"
            },
            instructions: `# Role
You are a voice-enabled assistant conducting behavioral interviews for software engineering positions.

# Pacing
SPEAK AT A FAST TEMPO (APPROX 1.25X). KEEP RESPONSES BRISK WITHOUT SOUNDING RUSHED.

# Output
Always answer aloud and include a concise text transcription.

# Interview Brief
You are a senior software engineering interviewer recruiting this candidate. Tailor each question to their background using the STAR method (Situation, Task, Action, Result).

# Resume Context
${resumeText.slice(0, 20000)}`,
          },
        };

        console.log("[Realtime] Sending session configuration:", sessionConfig);
        dataChannel.send(JSON.stringify(sessionConfig));

        // Send initial greeting trigger after a short delay
        setTimeout(() => {
          console.log("[Realtime] Sending response.create to start interview");
          const responseCreate = {
            type: "response.create",
            response: {
              instructions: "Please begin the software engineering behavioral interview now. Greet the candidate and ask your first question based on their resume."
            }
          };
          dataChannel.send(JSON.stringify(responseCreate));
        }, 1000);
      });

      dataChannel.addEventListener("message", (e) => {
        // try {
        //   const event = JSON.parse(e.data);
        //   console.log("[Realtime] Server event:", event.type, event);

        //   // Log full details for conversation items to debug user speech
        //   if (event.type === "conversation.item.created") {
        //     console.log("ðŸ” ITEM ROLE:", event.item?.role);
        //     console.log("ðŸ” ITEM TYPE:", event.item?.type);
        //     console.log("ðŸ” ITEM STATUS:", event.item?.status);
        //   }

        //   if (event.type === "session.updated") {
        //     console.log("[Realtime] Session updated successfully");
        //     onConnectionStatusChange("connected");
        //   } else if (event.type === "error") {
        //     console.error("[Realtime] Server error:", event);
        //     setError(event.error?.message || "Server error occurred");
        //   } else if (event.type === "conversation.item.input_audio_transcription.completed") {
        //     // User speech transcribed
        //     console.log("ðŸ—£ï¸ [Realtime] USER TRANSCRIPT:", event.transcript);
        //     console.log("ðŸ” onTranscript callback defined?", !!onTranscript);
        //     if (onTranscript && event.transcript) {
        //       console.log("âœ… Calling onTranscript with user transcript");
        //       onTranscript("user", event.transcript);
        //     } else {
        //       console.warn("âŒ NOT calling onTranscript - callback:", !!onTranscript, "transcript:", !!event.transcript);
        //     }
        //   } else if (event.type === "response.audio_transcript.delta") {
        //     // Assistant response transcription in progress
        //     console.log("[Realtime] Assistant transcript delta:", event.delta);
        //   } else if (event.type === "response.audio_transcript.done") {
        //     // Assistant response transcribed
        //     console.log("ðŸ¤– [Realtime] ASSISTANT TRANSCRIPT:", event.transcript);
        //     console.log("ðŸ” onTranscript callback defined?", !!onTranscript);
        //     if (onTranscript && event.transcript) {
        //       console.log("âœ… Calling onTranscript with assistant transcript");
        //       onTranscript("assistant", event.transcript);
        //     } else {
        //       console.warn("âŒ NOT calling onTranscript - callback:", !!onTranscript, "transcript:", !!event.transcript);
        //     }
        //   } else if (event.type === "response.done") {
        //     // Alternative: extract assistant transcript from response
        //     console.log("[Realtime] Response done:", event.response);
        //     if (onTranscript && event.response?.output) {
        //       for (const output of event.response.output) {
        //         if (output.type === "message" && output.content) {
        //           for (const content of output.content) {
        //             if (content.type === "audio" && content.transcript) {
        //               onTranscript("assistant", content.transcript);
        //             }
        //           }
        //         }
        //       }
        //     }
        //   } else if (event.type === "input_audio_buffer.speech_started") {
        //     console.log("ðŸŽ¤ [Realtime] User STARTED speaking - microphone is detecting audio!");
        //   } else if (event.type === "input_audio_buffer.speech_stopped") {
        //     console.log("ðŸŽ¤ [Realtime] User STOPPED speaking - processing speech...");
        //   } else if (event.type === "input_audio_buffer.committed") {
        //     console.log("âœ… [Realtime] Audio committed to conversation");
        //   } else if (event.type === "conversation.item.created") {
        //     console.log("ðŸ“ [Realtime] Conversation item created:", event.item);
        //     // Check if this is a user message
        //     if (event.item?.role === "user") {
        //       console.log("ðŸ‘¤ [Realtime] USER MESSAGE DETECTED!");
        //       console.log("Content:", event.item?.content);
        //       // Try to find transcript in different places
        //       if (event.item?.content) {
        //         const audioContent = event.item.content.find((c: any) => c.type === "input_audio");
        //         if (audioContent) {
        //           console.log("ðŸŽ¤ Audio content found:", audioContent);
        //           if (audioContent.transcript) {
        //             console.log("ðŸ—£ï¸ [Realtime] USER SAID:", audioContent.transcript);
        //             if (onTranscript) {
        //               onTranscript("user", audioContent.transcript);
        //             }
        //           }
        //         }
        //       }
        //     }
        //   } else if (event.type === "conversation.item.input_audio_transcription.completed") {
        //     console.log("ðŸ—£ï¸ [Realtime] USER TRANSCRIPT:", event.transcript);
        //     if (onTranscript && event.transcript) {
        //       onTranscript("user", event.transcript);
        //     }
        //   } else if (event.type === "conversation.item.input_audio_transcription.failed") {
        //     console.error("âŒ [Realtime] Transcription FAILED:", event.error);
        //   } else if (event.type === "response.created") {
        //     console.log("ðŸ’¬ [Realtime] AI Response created:", event.response);
        //   } else if (event.type === "response.output_item.added") {
        //     console.log("[Realtime] Output item added");
        //   } else if (event.type === "response.content_part.added") {
        //     console.log("[Realtime] Content part added");
        //   } else if (event.type === "response.audio.delta") {
        //     console.log("[Realtime] Audio delta received");
        //   } else if (event.type === "response.audio.done") {
        //     console.log("[Realtime] Audio done");
        //   }
        // } catch (err) {
        //   console.error("[Realtime] Failed to parse server message:", err);
        // }
      });

      dataChannel.addEventListener("error", (e) => {
        console.error("[Realtime] Data channel error:", e);
        setError("Connection error occurred");
      });

      dataChannel.addEventListener("close", () => {
        console.log("[Realtime] Data channel closed");
        cleanup();
      });

      // Handle connection state changes
      pc.addEventListener("connectionstatechange", () => {
        console.log("[Realtime] Connection state:", pc.connectionState);
        if (pc.connectionState === "failed" || pc.connectionState === "closed") {
          setError("Connection lost");
          cleanup();
        }
      });

      // Create offer
      console.log("[Realtime] Creating SDP offer...");
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      // Send offer to OpenAI Realtime API using ephemeral token
      console.log("[Realtime] Sending offer to OpenAI...");
      const model = "gpt-realtime";
      const response = await fetch(`https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}`, {
        method: "POST",
        headers: {
          Authorization: `Bearer ${token}`,
          "Content-Type": "application/sdp",
        },
        body: offer.sdp,
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Failed to connect: ${response.status} ${errorText}`);
      }

      // Get answer SDP
      const answerSdp = await response.text();
      console.log("[Realtime] Received answer from OpenAI");

      // Set remote description
      await pc.setRemoteDescription({
        type: "answer",
        sdp: answerSdp,
      });

      console.log("[Realtime] WebRTC connection established");
    } catch (err) {
      console.error("[Realtime] Failed to start interview:", err);
      setError(err instanceof Error ? err.message : "Failed to start interview");
      cleanup();
    }
  }, [resumeText, voice, onConnectionStatusChange, cleanup]);

  // Stop interview
  const stopInterview = useCallback(() => {
    console.log("[Realtime] Stopping interview...");
    cleanup();
  }, [cleanup]);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      cleanup();
    };
  }, [cleanup]);

  return {
    startInterview,
    stopInterview,
    audioAnalyser,
    error,
  };
};
